{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "#from sklearn.metrics import f1_score, roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "import torchvision\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lstdir=os.listdir(\\'/home/mar/Загрузки/benetech-making-graphs-accessible(1)/train/annotations\\')\\nfor i in lstdir:\\n    s=\"\"\\n    try:\\n            with open(i) as f:\\n                s=f.read()\\n    except:\\n            continue\\n    gen=json.loads(s)\\n    if len(gen[\\'visual-elements\\'][\\'boxplots\\'])!=0:\\n        print(i)\\n    del s'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''lstdir=os.listdir('/home/mar/Загрузки/benetech-making-graphs-accessible(1)/train/annotations')\n",
    "for i in lstdir:\n",
    "    s=\"\"\n",
    "    try:\n",
    "            with open(i) as f:\n",
    "                s=f.read()\n",
    "    except:\n",
    "            continue\n",
    "    gen=json.loads(s)\n",
    "    if len(gen['visual-elements']['boxplots'])!=0:\n",
    "        print(i)\n",
    "    del s'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interesting(path,dimensions):\n",
    "    def typization(string):\n",
    "        if string=='chart_title':\n",
    "            return (0,1,0)\n",
    "        elif string=='axis_title':\n",
    "            return (0,2,0)\n",
    "        elif string=='tick_label':\n",
    "            return (0,3,0)\n",
    "        elif string=='plot-bb':\n",
    "            return (0,4,0)\n",
    "        elif string=='axes':\n",
    "            return (0,5,0)\n",
    "        elif string==\"bars\":\n",
    "            return (0,6,0)\n",
    "        elif string==\"boxplots\":\n",
    "            return (0,7,0)\n",
    "        elif string==\"dot points\":\n",
    "            return (0,8,0)\n",
    "        elif string==\"lines\":\n",
    "            return (0,9,0)\n",
    "        elif string==\"scatter points\":\n",
    "            return (0,10,0)\n",
    "#    creator = np.array(dimensions[1]*[dimensions[2]*[0]])\n",
    "    creator = np.zeros((dimensions[1], dimensions[2]), dtype=np.uint8)\n",
    "    creator = cv2.cvtColor(creator, cv2.COLOR_GRAY2BGR)\n",
    "    print('img: ', dimensions,'\\n')\n",
    "    s=\"\"\n",
    "    with open(path) as f:\n",
    "        s=f.read()\n",
    "    gen=json.loads(s)\n",
    "    gen2=gen['plot-bb']\n",
    "    cv2.fillPoly(creator, [np.array([[(gen2['x0']+gen2['width'],gen2['y0'])],[(gen2['x0'],gen2['y0'])],[(gen2['x0'],gen2['y0']+gen2['height'])],[(gen2['x0']+gen2['width'],gen2['y0']+gen2['height'])]],dtype=np.int32)], typization('plot-bb'))\n",
    "    for i in range(len(gen['text'])):\n",
    "        if len(gen['text'][i]['polygon'])!=0:\n",
    "            gen1=gen['text'][i]['polygon']\n",
    "            cv2.fillPoly(creator, [np.array([[(gen1['x0'],gen1['y0'])],[(gen1['x1'],gen1['y1'])],[(gen1['x2'],gen1['y2'])],[(gen1['x3'],gen1['y3'])]],dtype=np.int32)], typization(gen['text'][i]['role']))\n",
    "    axe=gen['axes']\n",
    "    axex=axe['x-axis']['ticks']\n",
    "    axey=axe['y-axis']['ticks']\n",
    "    for i in range(len(axex)):\n",
    "        cv2.fillPoly(creator, [np.array([[(axex[i]['tick_pt']['x'],axex[i]['tick_pt']['y'])],[(axex[i]['tick_pt']['x'],axex[i]['tick_pt']['y']-gen2['height'])]],dtype=np.int32)], typization('axes'))\n",
    "    for i in range(len(axey)):\n",
    "        cv2.fillPoly(creator, [np.array([[(axey[i]['tick_pt']['x'],axey[i]['tick_pt']['y'])],[(axey[i]['tick_pt']['x']+gen2['width'],axey[i]['tick_pt']['y'])]],dtype=np.int32)], typization('axes'))\n",
    "    if len(gen[\"visual-elements\"][\"bars\"])!=0:\n",
    "        for el in gen[\"visual-elements\"][\"bars\"]:\n",
    "            cv2.fillPoly(creator, [np.array([[(el['x0']+el['width'],el['y0'])],[(el['x0'],el['y0'])],[(el['x0'],el['y0']+el['height'])],[(el['x0']+el['width'],el['y0']+el['height'])]],dtype=np.int32)], typization('bars'))\n",
    "    elif len(gen[\"visual-elements\"][\"boxplots\"])!=0:\n",
    "        ...\n",
    "    elif len(gen['visual-elements']['dot points'])!=0:\n",
    "        for i in gen['visual-elements']['dot points'][0]:\n",
    "            cv2.circle(creator, (int(i['x']), int(i['y'])), 4, typization('dot points'), 8)\n",
    "    elif len(gen['visual-elements']['lines'])!=0:\n",
    "        k=[]\n",
    "        for i in gen['visual-elements']['lines'][0]:\n",
    "            cv2.circle(creator, (int(i['x']), int(i['y'])), 2, typization('dot points'), 5)\n",
    "        for i in range(1,len(gen['visual-elements']['lines'][0])):\n",
    "            cv2.drawContours(creator,[np.array([(int(gen['visual-elements']['lines'][0][i-1]['x']),int(gen['visual-elements']['lines'][0][i-1]['y'])),(int(gen['visual-elements']['lines'][0][i]['x']),int(gen['visual-elements']['lines'][0][i]['y']))])],-1,typization('lines'),3)\n",
    "    elif len(gen['visual-elements']['scatter points'])!=0:\n",
    "        for i in gen['visual-elements']['scatter points'][0]:\n",
    "            cv2.circle(creator, (int(i['x']), int(i['y'])), 2, typization('dot points'), 5)\n",
    "    creator = cv2.cvtColor(creator, cv2.COLOR_BGR2GRAY)\n",
    "    creator = np.array([creator]*3)\n",
    "    output = torch.from_numpy(creator)\n",
    "    print('mask: ',output.shape)\n",
    "    return output\n",
    "#    plt.imshow(creator)\n",
    "#    plt.show()\n",
    "#interesting('/home/mar/Загрузки/benetech-making-graphs-accessible(1)/train/annotations/0294808fcbc0.json',(1,600,600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               id data_series    chart_type\n",
      "0  000b92c3b098_x     abc;def  vertical_bar\n",
      "1  000b92c3b098_y     0.0;1.0  vertical_bar\n",
      "2  007a18eb4e09_x     abc;def  vertical_bar\n",
      "3  007a18eb4e09_y     0.0;1.0  vertical_bar\n",
      "4  00dcf883a459_x     abc;def  vertical_bar\n",
      "['bars', 'boxplots', 'dot points', 'lines', 'scatter points']\n",
      "img:  torch.Size([3, 287, 497]) \n",
      "\n",
      "mask:  torch.Size([3, 287, 497])\n",
      "torch.Size([3, 287, 497])\n",
      "img:  torch.Size([3, 287, 497]) \n",
      "\n",
      "mask:  torch.Size([3, 287, 497])\n",
      "torch.Size([3, 287, 497])\n",
      "img:  torch.Size([3, 281, 466]) \n",
      "\n",
      "mask:  torch.Size([3, 281, 466])\n"
     ]
    }
   ],
   "source": [
    "image_path = '/home/mar/Загрузки/benetech-making-graphs-accessible(1)/train/images'\n",
    "mask_path = '/home/mar/Загрузки/benetech-making-graphs-accessible(1)/train/annotations'\n",
    "labels = pd.read_csv('/home/mar/Загрузки/benetech-making-graphs-accessible(1)/sample_submission.csv')\n",
    "print(labels.head())\n",
    "classes = ['bars','boxplots',\"dot points\",\"lines\",\"scatter points\"]#labels.name.values.tolist()\n",
    "print(classes)\n",
    "length = len(os.listdir(image_path))\n",
    "\n",
    "def img_transform(img, mask):\n",
    "\n",
    "    img = img.float() / 255.0\n",
    "\n",
    "    one_dim = img.shape[2]\n",
    "    two_dim = img.shape[3]\n",
    "    one_dim1 = mask.shape[2]\n",
    "    two_dim1 = mask.shape[3]\n",
    "\n",
    "    dim1 = torch.Tensor([[(4096-one_dim)*[[0]]]])\n",
    "    dim2 = torch.Tensor([[[(4096-two_dim)*[0]]]])\n",
    "    dim3 = torch.Tensor([[(4096-one_dim1)*[[0]]]])\n",
    "    dim4 = torch.Tensor([[[(4096-two_dim1)*[0]]]])\n",
    "\n",
    "    img = torch.cat((torch.cat((img,dim1),dim=2),dim2),dim=3)\n",
    "    mask = torch.cat((torch.cat((mask,dim3),dim=2),dim4),dim=3)\n",
    "    img = img.to(device)\n",
    "\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    return img, mask.squeeze(1).long()\n",
    "\n",
    "def img_transform1(img, mask):\n",
    "\n",
    "    img = img.float() / 255.0\n",
    "\n",
    "    one_dim = img.shape[1]\n",
    "    two_dim = img.shape[2]\n",
    "    one_dim1 = mask.shape[1]\n",
    "    two_dim1 = mask.shape[2]\n",
    "\n",
    "    dim1 = torch.Tensor([(4096-one_dim)*[[0]]])\n",
    "    dim2 = torch.Tensor([[(4096-two_dim)*[0]]])\n",
    "    dim3 = torch.Tensor([(4096-one_dim1)*[[0]]])\n",
    "    dim4 = torch.Tensor([[(4096-two_dim1)*[0]]])\n",
    "\n",
    "    img = torch.cat((torch.cat((img,dim1),dim=1),dim2),dim=2)\n",
    "    mask = torch.cat((torch.cat((mask,dim3),dim=1),dim4),dim=2)\n",
    "    img = img.to(device)\n",
    "\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    return img, mask#.squeeze(1).long()\n",
    "\n",
    "class Graphics(Dataset):\n",
    "    def __init__(self, imgs_dir, masks_dir, count, is_val = False):\n",
    "        self.imgs_dir = imgs_dir\n",
    "        self.masks_dir = masks_dir\n",
    "\n",
    "        imgs_paths = os.listdir(self.imgs_dir)\n",
    "        imgs_paths.sort()\n",
    "\n",
    "        mask_paths = os.listdir(self.masks_dir)\n",
    "        mask_paths.sort()\n",
    "\n",
    "        self.is_val = is_val\n",
    "        if not is_val:  # для разделения на train/val в процессе\n",
    "            self.imgs_paths = imgs_paths[:count]\n",
    "            self.mask_paths = mask_paths[:count]\n",
    "        else:\n",
    "            self.imgs_paths = imgs_paths[-count:]\n",
    "            self.mask_paths = mask_paths[-count:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = read_image(os.path.join(self.imgs_dir, self.imgs_paths[idx]), ImageReadMode.RGB)\n",
    "        mask = interesting(os.path.join(self.masks_dir, self.mask_paths[idx]),img.shape)\n",
    "#        mask = read_image(os.path.join(self.masks_dir, self.mask_paths[idx]), ImageReadMode.GRAY)\n",
    "\n",
    "        return img, mask#img_transform1(img, mask)\n",
    "    \n",
    "torchvision.models.segmentation.DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1.transforms()\n",
    "\n",
    "\n",
    "train_dataset_len = int(length * 0.7)\n",
    "val_dataset_len = length - train_dataset_len\n",
    "\n",
    "train_dataset = Graphics(image_path, mask_path, train_dataset_len)\n",
    "\n",
    "val_dataset = Graphics(image_path, mask_path, val_dataset_len, is_val=True)\n",
    "\n",
    "print(train_dataset[5][0].shape)\n",
    "print(train_dataset[5][1].shape)\n",
    "img, mask = next(iter(train_dataset))\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "model = torchvision.models.segmentation.deeplabv3_resnet50(weights=torchvision.models.segmentation.DeepLabV3_ResNet50_Weights.DEFAULT,\n",
    "                                                    progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model\u001b[39m.\u001b[39mclassifier \u001b[39m=\u001b[39m DeepLabHead(\u001b[39m2048\u001b[39m, \u001b[39m23\u001b[39m)\n\u001b[1;32m      6\u001b[0m model\u001b[39m.\u001b[39maux_classifier \u001b[39m=\u001b[39m FCNHead(\u001b[39m1024\u001b[39m, \u001b[39m23\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m CrossEntropyLoss\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "from torchvision.models.segmentation.fcn import FCNHead\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "model.classifier = DeepLabHead(2048, 23)\n",
    "model.aux_classifier = FCNHead(1024, 23)\n",
    "model = model.to(device)\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "loss = CrossEntropyLoss().to(device)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_accuracy(mask, output):\n",
    "    output_softmax = F.softmax(output, dim=1)\n",
    "    output_argmax = torch.argmax(output_softmax, dim=1)\n",
    "#softmax(zi) = exp(zi)/sum[k:1..K](exp(zk))\n",
    "#Свойства softmaxя\n",
    "    bool_tensor = (torch.flatten(mask)) == (torch.flatten(output_argmax))\n",
    "\n",
    "    return torch.sum(bool_tensor) / torch.numel(bool_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21202\n",
      "9087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21202 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img:  torch.Size([3, 272, 469]) \n",
      "\n",
      "mask:  torch.Size([3, 272, 469])\n",
      "img:  img: torch.Size([3, 276, 464])  \n",
      "torch.Size([3, 268, 464])\n",
      " \n",
      "\n",
      "mask:  mask: torch.Size([3, 276, 464]) \n",
      "torch.Size([3, 268, 464])\n",
      "img:  torch.Size([3, 290, 490]) \n",
      "\n",
      "mask:  torch.Size([3, 290, 490])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21202 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img:  torch.Size([3, 283, 504]) \n",
      "\n",
      "img: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/mar/.local/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mar/.local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mar/.local/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 264, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mar/.local/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mar/.local/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mar/.local/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mar/.local/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 161, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Trying to resize storage that is not resizable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     train_pixel_acc \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     27\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> 28\u001b[0m     \u001b[39mfor\u001b[39;00m img_batch, mask_batch \u001b[39min\u001b[39;00m tqdm(train_loader):\n\u001b[1;32m     29\u001b[0m \u001b[39m#        print(epoch)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         img_batch \u001b[39m=\u001b[39m img_batch\u001b[39m.\u001b[39mto(device, non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     31\u001b[0m         mask_batch \u001b[39m=\u001b[39m mask_batch\u001b[39m.\u001b[39mto(device, non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1373\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/mar/.local/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mar/.local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mar/.local/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 264, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mar/.local/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mar/.local/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mar/.local/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/mar/.local/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 161, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Trying to resize storage that is not resizable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask:   torch.Size([3, 283, 504])torch.Size([3, 347, 487]) \n",
      "\n",
      "\n",
      "mask: img:  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([3, 347, 487])torch.Size([3, 328, 501])\n",
      " \n",
      "\n",
      "mask: img:   torch.Size([3, 328, 501])torch.Size([3, 299, 509])\n",
      " \n",
      "\n",
      "mask:  torch.Size([3, 299, 509])\n",
      "img:  torch.Size([3, 275, 466]) \n",
      "\n",
      "mask:  torch.Size([3, 275, 466])\n",
      "img:  torch.Size([3, 480, 640]) \n",
      "\n",
      "mask:  torch.Size([3, 480, 640])\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "epoch_count = 30\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "es_steps = 3\n",
    "count_steps = 0\n",
    "\n",
    "train_len = len(train_loader)\n",
    "val_len = len(val_loader)\n",
    "print(train_len)\n",
    "print(val_len)\n",
    "\n",
    "best_score = 1e10\n",
    "\n",
    "for epoch in range(epoch_count):\n",
    "    if count_steps >= es_steps:\n",
    "        print(\"Early stopping!\")\n",
    "        break\n",
    "\n",
    "    train_loss_sum = 0\n",
    "    train_pixel_acc = 0\n",
    "\n",
    "    model.train()\n",
    "    for img_batch, mask_batch in tqdm(train_loader):\n",
    "#        print(epoch)\n",
    "        img_batch = img_batch.to(device, non_blocking=True)\n",
    "        mask_batch = mask_batch.to(device, non_blocking=True)\n",
    "        img_batch, mask_batch = img_transform(img_batch, mask_batch, is_val=False)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output_batch = model(img_batch)\n",
    "        loss_value = loss(output_batch['out'], mask_batch)\n",
    "\n",
    "        train_pixel_acc += pixel_accuracy(mask_batch, output_batch['out']).detach() #Обучить нейросеть с использованием pixel_accuracy в качестве loss-функции\n",
    "        train_loss_sum += loss_value.detach()\n",
    "\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        del output_batch\n",
    "\n",
    "    train_loss = train_loss_sum / train_len\n",
    "    train_acc = train_pixel_acc / train_len\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch} / {epoch_count} | train loss = {train_loss} | train acc = {train_acc}\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    val_loss_sum = 0\n",
    "    val_pixel_acc = 0\n",
    "\n",
    "    for img_batch, mask_batch in (val_loader):\n",
    "        img_batch = img_batch.to(device, non_blocking=True)\n",
    "        mask_batch = mask_batch.to(device, non_blocking=True)\n",
    "        img_batch, mask_batch = img_transform(img_batch, mask_batch, is_val=True)\n",
    "\n",
    "        output_batch = model(img_batch)\n",
    "        loss_value = loss(output_batch['out'], mask_batch)\n",
    "\n",
    "        val_loss_sum = val_loss_sum + loss_value.detach()\n",
    "        val_pixel_acc = val_pixel_acc + pixel_accuracy(mask_batch, output_batch['out']).detach()\n",
    "        \n",
    "        del output_batch\n",
    "\n",
    "    val_loss = val_loss_sum / val_len\n",
    "    val_acc = val_pixel_acc / val_len\n",
    "\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    print(f\"Epoch {epoch} / {epoch_count} | val loss = {val_loss} | val acc = {val_acc}\")\n",
    "\n",
    "    if val_loss < best_score:\n",
    "        best_score = val_loss\n",
    "        count_steps = 0\n",
    "        torch.save(model, \"best_model.pt\")\n",
    "    else:\n",
    "        count_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_losses = [x.cpu().item() for x in train_losses]\n",
    "val_losses = [x.cpu().item() for x in val_losses]\n",
    "plt.plot(train_losses, linestyle=\"-\")\n",
    "plt.plot(val_losses, linestyle=\"--\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accs = [x.cpu().item() for x in train_accs]\n",
    "val_accs = [x.cpu().item() for x in val_accs]\n",
    "\n",
    "plt.plot(train_accs, linestyle=\"-\")\n",
    "plt.plot(val_accs, linestyle=\"--\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "label_map = np.array([\n",
    "    (0, 0, 0),  # unlabeled\n",
    "    (128, 64, 128),  # chart_title\n",
    "    (130, 76, 0),  # axis_title\n",
    "    (0, 102, 0),  # tick_label\n",
    "    (112, 103, 87),  # plot-bb\n",
    "    (28, 42, 168),  # axes\n",
    "    (48, 41, 30),  # bars\n",
    "    (0, 50, 89), # boxplots\n",
    "    (107, 142, 35),  # dot points\n",
    "    (70, 70, 70),  # lines\n",
    "    (102, 102, 156),  # scatter points\n",
    "    (255, 0, 0), # conflicting\n",
    "])\n",
    "\n",
    "def draw_segmentation_map(outputs):\n",
    "    labels = torch.argmax(outputs.squeeze(), dim=0).numpy()\n",
    "  \n",
    "    # Create 3 Numpy arrays containing zeros.\n",
    "    # Later each pixel will be filled with respective red, green, and blue pixels\n",
    "    # depending on the predicted class.\n",
    "  \n",
    "    red_map   = np.zeros_like(labels).astype(np.uint8)\n",
    "    green_map = np.zeros_like(labels).astype(np.uint8)\n",
    "    blue_map  = np.zeros_like(labels).astype(np.uint8)\n",
    "  \n",
    "    for label_num in range(0, len(label_map)):\n",
    "        index = labels == label_num\n",
    "         \n",
    "        R, G, B = label_map[label_num]\n",
    "  \n",
    "        red_map[index]   = R\n",
    "        green_map[index] = G\n",
    "        blue_map[index]  = B\n",
    "  \n",
    "    segmentation_map = np.stack([red_map, green_map, blue_map], axis=2)\n",
    "    return segmentation_map\n",
    "\n",
    "def image_overlay(image, segmented_image):\n",
    "    alpha = 1  # transparency for the original image\n",
    "    beta  = 0.8  # transparency for the segmentation map\n",
    "    gamma = 0  # scalar added to each sum\n",
    "  \n",
    "    image = np.array(image)\n",
    "    segmented_image = cv2.cvtColor(segmented_image, cv2.COLOR_RGB2BGR)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "     \n",
    "    cv2.addWeighted(image, alpha, segmented_image, beta, gamma, image)\n",
    "  \n",
    "    return image\n",
    "\n",
    "imgs_paths = os.listdir(image_path)\n",
    "imgs_paths.sort()\n",
    "\n",
    "def perform_inference(model=model ,imgs_paths=imgs_paths, num_images=10, image_dir='/home/mar/Загрузки/benetech-making-graphs-accessible(1)/train/images/', device='cpu'):\n",
    "     \n",
    "     \n",
    "    device = device if device is not None else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    #preprocess = transforms.Compose([\n",
    "    #  transforms.Resize([520, 520]),\n",
    "    #  transforms.ToTensor(),\n",
    "    #  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    #])\n",
    "    # Load image handles for the validation set.\n",
    "  \n",
    "    # Randomly select 'num_images' from the whole set for inference.\n",
    "    selected_images = np.random.choice(imgs_paths, num_images, replace=False)\n",
    "  \n",
    "    # Iterate over selected images\n",
    "    for img_name in selected_images:\n",
    "         \n",
    "        # Load and pre-process image.\n",
    "        image_path = os.path.join(image_dir, img_name)\n",
    "        img_raw = Image.open(image_path).convert(\"RGB\")\n",
    "        W, H = img_raw.size[:2]\n",
    "        img_t = img_transform(img_raw)\n",
    "        img_t = torch.unsqueeze(img_t, dim=0).to(device)\n",
    "  \n",
    "        # Model Inference\n",
    "        with torch.no_grad():\n",
    "            output = model(img_t)[\"out\"].cpu()\n",
    "  \n",
    "        # Get RGB segmentation map\n",
    "        segmented_image = draw_segmentation_map(output)\n",
    "  \n",
    "        # Resize to original image size\n",
    "        segmented_image = cv2.resize(segmented_image, (W, H), cv2.INTER_LINEAR)\n",
    "        overlayed_image = image_overlay(img_raw, segmented_image)\n",
    "         \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 10), dpi=100)\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Image\")\n",
    "        plt.imshow(np.asarray(img_raw))\n",
    "  \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title(\"Segmentation\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(segmented_image)\n",
    "  \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(\"Overlayed\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(overlayed_image[:, :, ::-1])\n",
    "         \n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "  \n",
    "    return\n",
    "\n",
    "perform_inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
